\chapter{Marco Teórico}\label{chapter:state-of-the-art}

\section{Fundamentos de los Problemas de Satisfacción de Restricciones y SAT}
\label{sec:fundamentos-sat-csp}

Los Problemas de Satisfacción de Restricciones (CSPs) constituyen un paradigma fundamental para modelar desafíos combinatorios en inteligencia artificial, investigación operativa y ciencias de la computación. Formalmente, un CSP se define mediante una tripleta $(V,D,C)$, donde $V$ es un conjunto de variables, $D$ sus dominios discretos finitos, y $C$ un conjunto de restricciones que limitan las combinaciones válidas de valores \textbf{39}. Por ejemplo, en un problema de asignación de horarios, $V$ representaría cursos, $D$ los horarios disponibles, y $C$ las reglas que evitan superposiciones. La solución óptima no solo satisface todas las restricciones, sino que también optimiza criterios como la utilización de recursos \textbf{40}.

\subsection{SAT como Caso Especial de CSP y su NP-Completitud}
El Problema de la Satisfacibilidad Booleana (SAT) emerge como un CSP restringido donde los dominios son binarios (${0,1}$) y las restricciones se expresan en Forma Normal Conjuntiva (FNC) \textbf{43}. Una fórmula en FNC es una conjunción de cláusulas, donde cada cláusula es una disyunción de literales (variables o sus negaciones) \textbf{6}. Determinar si existe una asignación de valores que satisfaga todas las cláusulas equivale a resolver un CSP binario con restricciones específicas.

La relevancia teórica de SAT radica en su condición de problema NP-completo, demostrada por Cook y Levin en 1971 \textbf{2,24}. Este estatus implica dos consecuencias cruciales: primero, cualquier problema en NP puede reducirse a SAT en tiempo polinomial \textbf{26}; segundo, la existencia de un algoritmo polinomial para SAT implicaría $P=NP$, colapsando la jerarquía de complejidad \textbf{2}. Aunque en la práctica los solvers modernos resuelven instancias con millones de variables \textbf{3}, en el peor caso SAT conserva una complejidad exponencial inherente \textbf{27}.

\subsection{Ineficiencias Fundamentales de SAT}
\label{subsec:ineficiencia-sat}

El método de fuerza bruta para SAT —evaluar todas $2^n$ asignaciones posibles mediante tablas de verdad— ilustra su naturaleza intratable en el peor caso \textbf{19}. Esta explosión combinatoria se agrava en fórmulas sin estructura discernible, donde técnicas como la propagación unitaria o el aprendizaje de cláusulas tienen impacto limitado \textbf{30}. Por ejemplo, en instancias aleatorias de 3-SAT cerca del umbral de fase (aproximadamente 4.26 cláusulas por variable \textbf{30}), los algoritmos clásicos como DPLL exhiben un crecimiento exponencial en el tiempo de ejecución \textbf{27}.

Aun así, SAT destaca como herramienta práctica gracias a dos factores: (1) la capacidad de codificar CSPs genéricos en FNC mediante técnicas como \textit{encodings} directos o Tseitin \textbf{44}, y (2) el desarrollo de solvers CDCL que explotan regularidades empíricas en instancias industriales \textbf{3}. Esta dualidad entre dificultad teórica y éxito práctico sitúa a SAT en el núcleo de aplicaciones como verificación de hardware, planificación autónoma y criptoanálisis \textbf{39}.

\subsection{Relación Práctica entre CSPs y SAT}
\label{subsec:csp-sat-relacion}

Aunque los CSPs permiten modelar problemas con dominios arbitrarios y restricciones globales —ventaja frente a la rigidez booleana de SAT—, su resolución nativa mediante métodos como \textit{backtracking} o consistencia de arco sufre de limitaciones similares en escalabilidad \textbf{46}. Por ello, una estrategia común consiste en traducir CSPs a SAT, aprovechando décadas de optimizaciones en solvers CDCL \textbf{44}. Estudios empíricos demuestran que codificaciones eficientes (e.g., \textit{order encoding} para restricciones de orden) reducen hasta un 60\% el tiempo de resolución frente a enfoques CSP nativos \textbf{45}.

No obstante, esta traducción no está exenta de trade-offs. Mientras SAT favorece restricciones locales y cláusulas pequeñas, los CSPs manejan eficientemente restricciones globales (e.g., \textit{alldifferent}) mediante propagadores especializados \textbf{46}. Por ejemplo, en problemas de asignación de turnos hospitalarios, un modelo CSP con restricciones de recurso puede resolver instancias en minutos, mientras su traducción a SAT requiere horas debido a la explosión de cláusulas \textbf{46}. Esta dicotomía subraya la importancia de seleccionar el paradigma adecuado según la estructura del problema.

\subsection{Enfoques de Solución: CP, MILP y SAT}
\label{subsec:enfoques-solucion}

La resolución de CSPs y SAT se enmarca en tres metodologías principales:
\begin{itemize}
\item \textbf{Programación con Restricciones (CP)} \textbf{40}: Combina búsqueda sistemática con propagación de restricciones, ideal para dominios discretos y restricciones no lineales.
\item \textbf{Programación Entera Mixta (MILP)} \textbf{40}: Utiliza relajaciones lineales y técnicas branch-and-bound, óptima para problemas con estructura matemática explícita.
\item \textbf{Satisfacibilidad Booleana (SAT)} \textbf{44}: Emplea algoritmos CDCL con aprendizaje de cláusulas, eficaz para problemas binarios o altamente restringidos.
\end{itemize}

Cada enfoque tiene un nicho de aplicabilidad. Por ejemplo, CP domina en scheduling con restricciones complejas, MILP en optimización logística lineal, y SAT en verificación formal donde la traducibilidad a FNC es natural \textbf{39,40}. La elección depende críticamente de la capacidad para explotar la estructura subyacente del problema —factor que explica el éxito paradójico de SAT pese a su complejidad teórica \textbf{3}.

%\begin{figure}[h]
%\centering
%\includegraphics[width=0.85\textwidth]{sat-csp-comparison.pdf}
%\caption{Comparación de tiempos de resolución entre modelos CSP nativos y codificaciones SAT para instancias del problema N-Reinas (Fuente: Elaboración propia basada en benchmarks de \textbf{45}).}
%\label{fig:sat-csp-comp}
%\end{figure}

Esta sección sienta las bases para analizar en detalle la evolución de los SAT solvers (Sección \ref{sec:evolucion-sat-solvers}), donde se explorará cómo técnicas como CDCL superaron las ineficiencias teóricas mediante innovaciones algorítmicas pragmáticas.

\section{Evolución de los SAT \textit{solvers}: Principio de Resolución, DP, DPLL y CDCL}
\label{sec:evolucion-sat-solvers}

La historia de los SAT \textit{solvers} representa un hito en la ciencia computacional, transformando un problema teóricamente intratable en una herramienta práctica de amplio uso industrial. Esta evolución se sustenta en tres pilares algorítmicos: el Principio de Resolución, los métodos Davis-Putnam (DP) y Davis-Putnam-Logemann-Loveland (DPLL), y la revolución moderna impulsada por los solucionadores de aprendizaje de cláusulas dirigido por conflictos (CDCL). Este recorrido no solo refleja avances técnicos, sino una comprensión profunda de cómo combinar teoría y pragmatismo para superar las barreras de complejidad inherentes al problema de la satisfacibilidad booleana (SAT)\footnote{Problema NP-completo que busca determinar si existe una asignación de verdad que satisfaga una fórmula lógica dada.}.

\subsection{Principio de Resolución: Fundamento Teórico}
El Principio de Resolución (PR), propuesto inicialmente en el contexto de la lógica proposicional, constituye la base teórica de muchos algoritmos SAT. Este principio permite derivar nuevas cláusulas a partir de un par de cláusulas existentes que contengan literales complementarios, reduciendo progresivamente la fórmula hasta detectar una contradicción o verificar su satisfacibilidad. Aunque teóricamente completo para fórmulas en Forma Normal Conjuntiva (FNC), su aplicación directa resulta impráctica debido al crecimiento explosivo en el número de cláusulas generadas \textbf{12}. No obstante, su valor conceptual sentó las bases para métodos más refinados.

\subsection{Algoritmo Davis-Putnam (DP): Primer Intento Práctico}
Introducido en 1960 \textbf{5}, el algoritmo DP fue el primer esfuerzo sistemático para operacionalizar el PR en un marco computacional. Su estrategia se centraba en la eliminación iterativa de variables mediante dos reglas: (1) \textit{eliminación de literales puros} (asignar valores a variables que aparecen con un único signo en todas las cláusulas) y (2) \textit{resolución dirigida} para eliminar variables seleccionadas. Aunque evitaba el espacio exponencial de las tablas de verdad, su dependencia de la resolución lo hacía vulnerable a explosiones combinatorias en el número de cláusulas intermedias \textbf{12}. Este defecto limitó su aplicabilidad a instancias pequeñas, pero demostró que la combinación de inferencia lógica y manipulación algebraica podía abordar SAT de manera estructurada.

\subsection{Algoritmo DPLL: Búsqueda Inteligente con Retroceso}
En 1962, Davis, Putnam, Logemann y Loveland refinaron DP mediante un giro paradigmático: reemplazaron la resolución explícita por una \textit{búsqueda con retroceso} en el espacio de asignaciones, dando origen al algoritmo DPLL \textbf{12}. Este método integra tres componentes clave:
\begin{itemize}
\item \textbf{Propagación Unitaria}: Asignación forzada de literales en cláusulas unitarias, reduciendo la fórmula antes de tomar decisiones.
\item \textbf{Eliminación de Literales Puros}: Optimización heredada de DP para simplificar la fórmula.
\item \textbf{Bifurcación con Retroceso}: Elección heurística de valores para variables no determinadas, seguida de retroceso ante conflictos.
\end{itemize}

Al operar como un recorrido en profundidad del árbol de decisiones, DPLL evita el costo de memoria de DP y se convirtió en el núcleo de los SAT \textit{solvers} clásicos \textbf{7}. Sin embargo, su eficiencia se veía mermada en problemas industriales debido a la exploración redundante de subespacios conflictivos y la ausencia de mecanismos para capitalizar información de conflictos pasados \textbf{8}. Estas limitaciones motivaron la búsqueda de estrategias que trascendieran el paradigma de fuerza bruta.

\subsection{CDCL: La Revolución de los \textit{Solvers} Modernos}
\label{subsec:cdcl}

El algoritmo CDCL emergió como solución a las limitaciones estructurales de DPLL en instancias industriales. Dos innovaciones fueron fundamentales: el \textit{aprendizaje de cláusulas} y el \textit{retroceso no cronológico} \textbf{12}. El primero transforma los conflictos en conocimiento estructural mediante el análisis de implicaciones, generando cláusulas que encapsulan inconsistencias para podar futuras ramas de búsqueda \textbf{17}. El segundo permite saltar a niveles de decisión estratégicos, evitando la exploración redundante de subárboles irrelevantes \textbf{6}. Juntos, estos mecanismos resolvieron tres problemas críticos de DPLL: la explosión combinatoria por conflictos repetidos, la falta de adaptación heurística durante la búsqueda y la ineficiencia del retroceso cronológico \textbf{5,7,8}.

\subsubsection{Mejoras Clave en CDCL: Heurísticas y Estrategias}
\label{subsubsec:mejoras-cdcl}

Si bien CDCL superó teóricamente a DPLL, su eficacia práctica requirió mejoras adicionales. Entre ellas destacan las heurísticas de selección de variables, estrategias de reinicio adaptativo y optimizaciones en la propagación unitaria.

Las \textit{heurísticas de selección de variables} representan un equilibrio entre complejidad teórica y pragmatismo. DLIS (\textit{Dynamic Largest Individual Sum}), uno de los primeros enfoques dinámicos \textbf{17}, prioriza variables frecuentes en cláusulas insatisfechas mediante conteos estáticos \textbf{18}. Aunque intuitivo, su costo computacional lineal ($O(n)$) y su incapacidad para adaptarse a la dinámica de búsqueda lo limitaron en instancias industriales \textbf{22}. VSIDS (\textit{Variable State Independent Decaying Sum}), en contraste, introdujo un modelo basado en actividades dinámicas con decaimiento exponencial \textbf{27}. Al incrementar puntuaciones de variables en conflictos recientes y aplicar factores de olvido periódicos \textbf{28}, VSIDS logró un balance empírico entre adaptación y costo computacional ($O(1)$ mediante colas de prioridad) \textbf{30}. Sin embargo, ambas heurísticas son aproximaciones: DLIS por su simplicidad estática, VSIDS por depender de correlaciones no garantizadas entre conflictos y tiempo de resolución \textbf{10,35}.

Las \textit{estrategias de reinicio adaptativo} surgieron para contrarrestar el estancamiento en regiones locales del espacio de búsqueda, un efecto colateral de heurísticas como VSIDS \textbf{44}. Técnicas como los reinicios LBD-driven en Glucose \textbf{14} vinculan la frecuencia de reinicios al Literal Block Distance promedio de cláusulas aprendidas, indicador indirecto de estancamiento. Métodos más sofisticados, como MAB (\textit{Multi-Armed Bandit}) en Kissat \textbf{37}, emplean aprendizaje en línea para seleccionar políticas de reinicio óptimas según el contexto de búsqueda. Estas estrategias reducen hasta un 70\% el tiempo de resolución en instancias modulares al reorientar la exploración sin perder cláusulas aprendidas críticas \textbf{21}.

En la \textit{propagación unitaria}, optimizaciones como el esquema de Dos Literales Vigilados (TWL) \textbf{29} minimizaron el costo de detectar cláusulas unitarias, evitando inspeccionar el 90\% de las cláusulas en cada asignación \textbf{12}. Técnicas posteriores como CFUP (\textit{Core First Unit Propagation}) \textbf{96} priorizaron cláusulas con LBD $\leq$ 7, basándose en la observación empírica de que el 80\% de los conflictos ocurren en estas estructuras. Si bien CFUP carece de fundamentación teórica, su efectividad práctica lo consolidó como estándar en solvers como CaDiCaL \textbf{30}.

\subsubsection{Otras Contribuciones Relevantes}
\label{subsubsec:otras-mejoras}

El ecosistema de mejoras en CDCL incluye avances complementarios. La gestión de cláusulas aprendidas mediante métricas como LBD permitió eliminar cláusulas redundantes y reciclar aquellas con alto valor predictivo (\textit{glue clauses}) \textbf{14-17}. La integración con búsqueda local, mediante técnicas como \textit{re-phasing} basado en frecuencias de conflicto \textbf{1}, mejoró hasta un 40\% el rendimiento en instancias satisfacibles. En paralelo, meta-heurísticas como AutoSAT \textbf{35} exploraron el uso de modelos de lenguaje grande (LLMs) para optimizar parámetros dinámicos, aunque su adopción práctica sigue siendo incipiente.

%\begin{figure}[h]
%\centering
%\includegraphics[width=0.8\textwidth]{cdcl-evolucion.pdf}
%\caption{Reducción acumulada en tiempo de ejecución atribuible a mejoras en CDCL (Fuente: Análisis de resultados de SAT Competition 2010-2023).}
%\label{fig:cdcl-evol}
%\end{figure}

\subsection{CDCL como Motor para Problemas con Restricciones}
La efectividad de CDCL en CSPs depende críticamente de su arquitectura adaptativa. VSIDS explota patrones emergentes en restricciones codificadas, mientras los reinicios LBD-driven manejan la modularidad típica de estos problemas \textbf{15,21}. Técnicas como CFUP, al priorizar cláusulas "core", reflejan jerarquías implícitas en redes de restricciones \textbf{96}. Esta sinergia explica por qué solvers como Kissat y Glucose se han convertido en motores subyacentes para sistemas de verificación y planificación industrial \textbf{12}.

%\subsection{CDCL como Motor para Problemas con Restricciones}
%La versatilidad de CDCL trasciende SAT: al codificar Problemas de Satisfacción de Restricciones (CSPs) en FNC, estos \textit{solvers} actúan como oráculos para la clase NP \textbf{8}. Su capacidad para generar \textit{testigos} (soluciones) o \textit{núcleos insatisfacibles} (subconjuntos conflictivos) los hace herramientas clave en verificación formal, planificación y optimización combinatoria. Además, técnicas como el aprendizaje de cláusulas han influido en el diseño de solucionadores híbridos que integran SAT con dominios específicos \textbf{12} La elección de heurísticas como VSIDS y políticas de reinicio adaptativo resulta crucial al codificar CSPs en SAT, donde la estructura de restricciones se traduce en patrones específicos de aparición de variables que estas técnicas explotan \textbf{15,22}..

\subsection{Conclusión: De la Teoría a la Revolución Práctica}
La transición desde el PR y DP hasta CDCL ilustra cómo innovaciones algorítmicas pueden superar barreras teóricas aparentemente infranqueables. Mientras DPLL sentó las bases de la integración búsqueda-inferencia, CDCL introdujo un paradigma de \textit{aprendizaje reflexivo}, donde cada conflicto alimenta la inteligencia del solver. Esta evolución, respaldada por mejoras en heurísticas y estructuras de datos, ha convertido a los SAT \textit{solvers} en pilares de la computación moderna, demostrando que incluso problemas NP-completos pueden abordarse eficientemente en escenarios prácticos \textbf{5,7,11}.


\section{Parámetros de Evaluación para Solvers CDCL: Taxonomía de \textit{Benchmarks}}
\label{sec:tipos-problemas}

La evaluación rigurosa de solvers CDCL —particularmente en el análisis comparativo de heurísticas como VSIDS y DLIS, estrategias de reinicio y selección de cláusulas unitarias— requiere una taxonomía precisa de instancias SAT. Estas se clasifican no solo por su origen, sino por propiedades intrínsecas que revelan fortalezas y limitaciones algorítmicas \textbf{15,17}.

\subsection{Clasificación por Origen y Estructura}
Las competiciones anuales de SAT (\textit{SAT Competition}) establecen cuatro categorías canónicas \textbf{17}:

\textbf{Instancias de Aplicación}: Derivadas de problemas industriales como verificación de circuitos o planificación logística \textbf{11}, exhiben estructuras modulares y \textit{backdoors} pequeños —subconjuntos de variables cuya asignación correcta simplifica drásticamente la solución \textbf{21,23}. Estas características permiten a heurísticas dinámicas como VSIDS explotar patrones locales mediante el rastreo de conflictos recientes \textbf{20}.

\textbf{Instancias Combinatorias Dificultosas}: Construidas artificialmente para desafiar a los solvers (e.g., codificaciones del principio del palomar) \textbf{16}, carecen de la estructura implícita de las aplicaciones reales pero poseen simetrías y dependencias globales que prueban la capacidad de generalización de las heurísticas \textbf{19}.

\textbf{Instancias Aleatorias}: Generadas mediante modelos como Random k-SAT \textbf{17}, su falta de estructura las convierte en un desafío para CDCL, donde técnicas como el aprendizaje de cláusulas muestran eficacia limitada \textbf{21}. Estas instancias son críticas para evaluar el desempeño en ausencia de sesgos estructurales.

\textbf{Instancias Ágiles}: Resultantes de la conversión de problemas de lógicas superiores a SAT (\textit{bit-blasting}) \textbf{19}, testean la capacidad de manejar fórmulas con alta densidad de cláusulas y variables auxiliares.

\subsection{Clasificación por Satisfacibilidad y Propiedades Estructurales}
La naturaleza SAT o UNSAT de una instancia influye significativamente en el comportamiento del solver. Mientras las instancias SAT benefician a heurísticas que priorizan la exploración de asignaciones prometedoras (e.g., VSIDS con \textit{phase saving}), las UNSAT requieren estrategias que aceleren la refutación mediante cláusulas aprendidas de alto impacto \textbf{41}.

Propiedades estructurales como la modularidad, \textit{treewidth} y presencia de \textit{backbones} (variables con valor fijo en todas las soluciones) \textbf{20,22} ofrecen insights adicionales. Por ejemplo, instancias con alta modularidad —típicas en aplicaciones industriales— permiten a los reinicios adaptativos reorientar la búsqueda hacia comunidades no exploradas \textbf{21}, mientras que un \textit{treewidth} bajo correlaciona con tiempos de resolución reducidos debido a la eficiencia de la propagación unitaria \textbf{20}.

\subsection{Densidad y Transición de Fase}
La relación cláusulas/variables (densidad) determina la dificultad en instancias aleatorias. Para 3-SAT, la transición de fase alrededor de 4.27 cláusulas por variable \textbf{25} marca el pico de complejidad, donde métodos como DLIS —basados en frecuencias estáticas— fallan ante la ausencia de patrones explotables \textbf{17}. En contraste, VSIDS combinado con reinicios dinámicos (\textit{LBD-driven}) logra navegar estas regiones mediante un balance entre explotación local y exploración global \textbf{21}.

En instancias estructuradas, sin embargo, densidades altas no siempre implican mayor dificultad. Codificaciones compactas de problemas como el coloreado de grafos pueden ser resueltas eficientemente si los \textit{solvers} identifican \textit{backdoors} mediante heurísticas sensibles al contexto \textbf{23}.

\subsection{Relevancia para la Evaluación de Heurísticas}
La elección de \textit{benchmarks} es crucial al comparar técnicas como VSIDS y DLIS. Mientras DLIS —dependiente de conteos estáticos de aparición— rinde mejor en instancias aleatorias lejos de la transición de fase \textbf{18}, VSIDS domina en aplicaciones reales gracias a su adaptación dinámica \textbf{30}. Estrategias de reinicio como MAB \textbf{37} muestran ventajas en instancias modulares, donde reiniciar preservando cláusulas \textit{glue} acelera la cobertura del espacio de búsqueda \textbf{21}.

Propiedades como la presencia de \textit{backbones} o bajo \textit{treewidth} permiten diseccionar el impacto de técnicas específicas. Por ejemplo, la selección de cláusulas unitarias mediante CFUP \textbf{96} reduce hasta un 40\% el tiempo en instancias con alta densidad de cláusulas \textit{core} ($LBD \leq 7$), pero tiene efecto marginal en fórmulas sin estructura comunitaria \textbf{21}.

%\begin{figure}[h]
%\centering
%\includegraphics[width=0.85\textwidth]{benchmark-heuristicas.pdf}
%\caption{Desempeño comparativo de VSIDS vs DLIS en diferentes clases de \textit{benchmarks} (Fuente: Análisis de resultados de SAT Competition 2018-2023).}
%\label{fig:heuristicas-bench}
%\end{figure}

Este marco taxonómico no solo sistematiza la evaluación de solvers, sino que guía el diseño de heurísticas híbridas. Por ejemplo, la integración de VSIDS con métricas de centralidad de grafos \textbf{22} ha demostrado mejorar la cobertura en instancias combinatorias, mientras el acoplamiento con búsqueda local \textbf{1} beneficia a instancias ágiles. La elección estratégica de \textit{benchmarks} representa, así, un puente entre la complejidad teórica de SAT y su aplicabilidad práctica.




%En resumen, la eficiencia de heurísticas como VSIDS se evalúa en solvers CDCL completos utilizando benchmarks de SAT Competition que representan problemas del mundo real (Aplicación), problemas construidos para ser difíciles (Combinatorios) y, en menor medida para CDCL, problemas aleatorios. Los problemas se clasifican por origen, satisfacibilidad y propiedades estructurales como densidad, modularidad y la existencia de "backdoors". La densidad es particularmente relevante en la zona de transición de fase de las instancias aleatorias.









